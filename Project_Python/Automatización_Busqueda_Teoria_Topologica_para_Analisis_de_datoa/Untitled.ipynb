{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d614c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'container_type': 'Author', 'filled': [], 'source': <AuthorSource.SEARCH_AUTHOR_SNIPPETS: 'SEARCH_AUTHOR_SNIPPETS'>, 'scholar_id': 'WCkRu_kAAAAJ', 'url_picture': 'https://scholar.google.com/citations?view_op=medium_photo&user=WCkRu_kAAAAJ', 'name': 'Rafael Martínez Fonseca', 'affiliation': 'PhD student in mathematics', 'email_domain': '@mail.uniatlantico.edu.co', 'interests': ['Biomatemáticas'], 'citedby': 2}\n"
     ]
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "\n",
    "query = scholarly.search_author('Rafael Martinez Fonseca')\n",
    "author = next(query)\n",
    "print(author)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7af8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from scholarly import scholarly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5e3e5cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "MaxTriesExceededException",
     "evalue": "Cannot Fetch from Google Scholar.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMaxTriesExceededException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopological data analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m OR \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlandscape persistence diagrams\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) AND (\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeuroimage\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m OR \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMagnetic Resonance Image\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) AND (\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence interval\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m OR \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimation\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Realizar la búsqueda\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m search_results \u001b[38;5;241m=\u001b[39m \u001b[43mscholarly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_pubs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Crear y abrir un archivo CSV\u001b[39;00m\n\u001b[1;32m      8\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscholarly_search_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documentos/scholarly_env/lib/python3.12/site-packages/scholarly/_scholarly.py:160\u001b[0m, in \u001b[0;36m_Scholarly.search_pubs\u001b[0;34m(self, query, patents, citations, year_low, year_high, sort_by, include_last_year, start_index)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Searches by query and returns a generator of Publication objects\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m:param query: terms to be searched\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_url(_PUBSEARCH\u001b[38;5;241m.\u001b[39mformat(requests\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mquote(query)), patents\u001b[38;5;241m=\u001b[39mpatents,\n\u001b[1;32m    158\u001b[0m                           citations\u001b[38;5;241m=\u001b[39mcitations, year_low\u001b[38;5;241m=\u001b[39myear_low, year_high\u001b[38;5;241m=\u001b[39myear_high,\n\u001b[1;32m    159\u001b[0m                           sort_by\u001b[38;5;241m=\u001b[39msort_by, include_last_year\u001b[38;5;241m=\u001b[39minclude_last_year, start_index\u001b[38;5;241m=\u001b[39mstart_index)\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__nav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_publications\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/scholarly_env/lib/python3.12/site-packages/scholarly/_navigator.py:296\u001b[0m, in \u001b[0;36mNavigator.search_publications\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_publications\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _SearchScholarIterator:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a Publication Generator given a url\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    :param url: the url where publications can be found.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    :rtype: {_SearchScholarIterator}\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_SearchScholarIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/scholarly_env/lib/python3.12/site-packages/scholarly/publication_parser.py:53\u001b[0m, in \u001b[0;36m_SearchScholarIterator.__init__\u001b[0;34m(self, nav, url)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pubtype \u001b[38;5;241m=\u001b[39m PublicationSource\u001b[38;5;241m.\u001b[39mPUBLICATION_SEARCH_SNIPPET \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scholar?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m url \u001b[38;5;28;01melse\u001b[39;00m PublicationSource\u001b[38;5;241m.\u001b[39mJOURNAL_CITATION_LIST\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nav \u001b[38;5;241m=\u001b[39m nav\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_total_results()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_parser \u001b[38;5;241m=\u001b[39m PublicationParser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nav)\n",
      "File \u001b[0;32m~/Documentos/scholarly_env/lib/python3.12/site-packages/scholarly/publication_parser.py:59\u001b[0m, in \u001b[0;36m_SearchScholarIterator._load_url\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_url\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# this is temporary until setup json file\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_soup\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgs_r gs_or gs_scl\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgsc_mpat_ttl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/scholarly_env/lib/python3.12/site-packages/scholarly/_navigator.py:239\u001b[0m, in \u001b[0;36mNavigator._get_soup\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_soup\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BeautifulSoup:\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     html \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_page\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://scholar.google.com\u001b[39;49m\u001b[38;5;132;43;01m{0}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     html \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    241\u001b[0m     res \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/scholarly_env/lib/python3.12/site-packages/scholarly/_navigator.py:190\u001b[0m, in \u001b[0;36mNavigator._get_page\u001b[0;34m(self, pagerequest, premium)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_page(pagerequest, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxTriesExceededException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot Fetch from Google Scholar.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mMaxTriesExceededException\u001b[0m: Cannot Fetch from Google Scholar."
     ]
    }
   ],
   "source": [
    "\n",
    "# Palabras clave de la búsqueda\n",
    "query = '(\"Topological data analysis\" OR \"landscape persistence diagrams\") AND (\"Neuroimage\" OR \"Magnetic Resonance Image\") AND (\"confidence interval\" OR \"Estimation\")'\n",
    "\n",
    "# Realizar la búsqueda\n",
    "search_results = scholarly.search_pubs(query)\n",
    "\n",
    "# Crear y abrir un archivo CSV\n",
    "filename = \"scholarly_search_results.csv\"\n",
    "with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Escribir encabezados\n",
    "    writer.writerow([\"Título\", \"Autores\", \"Resumen\", \"Año\", \"URL\"])\n",
    "\n",
    "    # Recorrer resultados y escribir en el archivo\n",
    "    for i, result in enumerate(search_results):\n",
    "        if i >= 500:  # Cambia este número para controlar el número de artículos a guardar\n",
    "            break\n",
    "        try:\n",
    "            # Extraer datos relevantes\n",
    "            title = result.get(\"bib\", {}).get(\"title\", \"Sin título\")\n",
    "            authors = \", \".join(result.get(\"bib\", {}).get(\"author\", []))\n",
    "            abstract = result.get(\"bib\", {}).get(\"abstract\", \"No disponible\")\n",
    "            year = result.get(\"bib\", {}).get(\"pub_year\", \"No especificado\")\n",
    "            url = result.get(\"eprint_url\", \"No disponible\")\n",
    "\n",
    "            # Escribir en el CSV\n",
    "            writer.writerow([title, authors, abstract, year, url])\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar un artículo: {e}\")\n",
    "\n",
    "print(f\"Resultados guardados en el archivo {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e479505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://link.springer.com/search?query=Topological+data+analysis&page=1\n",
      "Scraping: https://link.springer.com/search?query=Topological+data+analysis&page=2\n",
      "Scraping: https://link.springer.com/search?query=Topological+data+analysis&page=3\n",
      "Resultados guardados en springer_results.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Función para construir la URL de búsqueda\n",
    "def build_springer_url(query, page=1):\n",
    "    base_url = \"https://link.springer.com/search\"\n",
    "    return f\"{base_url}?query={query.replace(' ', '+')}&page={page}\"\n",
    "\n",
    "# Encabezados para simular un navegador\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Consulta de búsqueda\n",
    "query = \"Topological data analysis\"\n",
    "results = []\n",
    "\n",
    "# Iterar sobre las páginas de resultados\n",
    "for page in range(1, 4):  # Cambia 4 al número de páginas que quieres procesar\n",
    "    url = build_springer_url(query, page)\n",
    "    print(f\"Scraping: {url}\")\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Encontrar todos los artículos en la página\n",
    "        articles = soup.select(\".result-item-content\")\n",
    "        for article in articles:\n",
    "            # Título del artículo\n",
    "            title_element = article.select_one(\".title\")\n",
    "            title = title_element.text.strip() if title_element else \"No title available\"\n",
    "            \n",
    "            # Enlace al artículo\n",
    "            link = f\"https://link.springer.com{title_element['href']}\" if title_element else None\n",
    "            \n",
    "            # Resumen (si está disponible)\n",
    "            snippet_element = article.select_one(\".snippet\")\n",
    "            snippet = snippet_element.text.strip() if snippet_element else \"No snippet available\"\n",
    "            \n",
    "            # Guardar el resultado\n",
    "            results.append({\"Title\": title, \"Link\": link, \"Snippet\": snippet})\n",
    "        \n",
    "        time.sleep(2)  # Pausa entre solicitudes para evitar ser bloqueado\n",
    "    else:\n",
    "        print(f\"Error al acceder a la página: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "# Guardar resultados en un archivo CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"springer_results.csv\", index=False)\n",
    "print(\"Resultados guardados en springer_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9d999e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.sciencedirect.com/search?qs=Topological+data+analysis&offset=0\n",
      "Error al acceder a la página: 403\n",
      "Resultados guardados en sciencedirect_results.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Función para construir la URL de búsqueda\n",
    "def build_sciencedirect_url(query, page=0):\n",
    "    base_url = \"https://www.sciencedirect.com/search\"\n",
    "    return f\"{base_url}?qs={query.replace(' ', '+')}&offset={page * 25}\"\n",
    "\n",
    "# Encabezados para simular un navegador\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Consulta de búsqueda\n",
    "query = \"Topological data analysis\"\n",
    "results = []\n",
    "\n",
    "# Iterar sobre las páginas de resultados\n",
    "for page in range(3):  # Cambia 3 al número de páginas que quieres procesar\n",
    "    url = build_sciencedirect_url(query, page)\n",
    "    print(f\"Scraping: {url}\")\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Encontrar todos los artículos en la página\n",
    "        articles = soup.select(\".result-item-content\")\n",
    "        for article in articles:\n",
    "            # Título del artículo\n",
    "            title_element = article.select_one(\".result-list-title-link\")\n",
    "            title = title_element.text.strip() if title_element else \"No title available\"\n",
    "            \n",
    "            # Enlace al artículo\n",
    "            link = f\"https://www.sciencedirect.com{title_element['href']}\" if title_element else None\n",
    "            \n",
    "            # Resumen (si está disponible)\n",
    "            snippet_element = article.select_one(\".result-list-snippet\")\n",
    "            snippet = snippet_element.text.strip() if snippet_element else \"No snippet available\"\n",
    "            \n",
    "            # Autores (si están disponibles)\n",
    "            authors_element = article.select_one(\".Authors\")\n",
    "            authors = authors_element.text.strip() if authors_element else \"No authors available\"\n",
    "            \n",
    "            # Guardar el resultado\n",
    "            results.append({\n",
    "                \"Title\": title,\n",
    "                \"Link\": link,\n",
    "                \"Snippet\": snippet,\n",
    "                \"Authors\": authors\n",
    "            })\n",
    "        \n",
    "        time.sleep(2)  # Pausa entre solicitudes para evitar ser bloqueado\n",
    "    else:\n",
    "        print(f\"Error al acceder a la página: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "# Guardar resultados en un archivo CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"sciencedirect_results.csv\", index=False)\n",
    "print(\"Resultados guardados en sciencedirect_results.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e786052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://pubmed.ncbi.nlm.nih.gov/?term=Topological+data+analysis&page=1\n",
      "Scraping: https://pubmed.ncbi.nlm.nih.gov/?term=Topological+data+analysis&page=2\n",
      "Scraping: https://pubmed.ncbi.nlm.nih.gov/?term=Topological+data+analysis&page=3\n",
      "Resultados guardados en pubmed_results.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Función para construir la URL de búsqueda\n",
    "def build_pubmed_url(query, page=1):\n",
    "    base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
    "    return f\"{base_url}?term={query.replace(' ', '+')}&page={page}\"\n",
    "\n",
    "# Encabezados para simular un navegador\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Consulta de búsqueda\n",
    "query = \"Topological data analysis\"\n",
    "results = []\n",
    "\n",
    "# Iterar sobre las páginas de resultados\n",
    "for page in range(1, 4):  # Cambia 4 al número de páginas que quieras procesar\n",
    "    url = build_pubmed_url(query, page)\n",
    "    print(f\"Scraping: {url}\")\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Encontrar todos los artículos en la página\n",
    "        articles = soup.select(\".docsum-content\")\n",
    "        for article in articles:\n",
    "            # Título del artículo\n",
    "            title_element = article.select_one(\".docsum-title\")\n",
    "            title = title_element.text.strip() if title_element else \"No title available\"\n",
    "            \n",
    "            # Enlace al artículo\n",
    "            link = f\"https://pubmed.ncbi.nlm.nih.gov{title_element['href']}\" if title_element else None\n",
    "            \n",
    "            # Autores (si están disponibles)\n",
    "            authors_element = article.select_one(\".docsum-authors.full-authors\")\n",
    "            authors = authors_element.text.strip() if authors_element else \"No authors available\"\n",
    "            \n",
    "            # Resumen (no está directamente en la lista de resultados, requiere visitar el enlace)\n",
    "            snippet = \"Abstract not available in search results\"\n",
    "            \n",
    "            # Guardar el resultado\n",
    "            results.append({\n",
    "                \"Title\": title,\n",
    "                \"Link\": link,\n",
    "                \"Authors\": authors,\n",
    "                \"Snippet\": snippet\n",
    "            })\n",
    "        \n",
    "        time.sleep(2)  # Pausa entre solicitudes para evitar ser bloqueado\n",
    "    else:\n",
    "        print(f\"Error al acceder a la página: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "# Guardar resultados en un archivo CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"pubmed_results.csv\", index=False)\n",
    "print(\"Resultados guardados en pubmed_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88138205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://pubmed.ncbi.nlm.nih.gov/?term=((\"Topological+data+analysis\"[Title/Abstract]+OR+\"landscape+persistence+diagrams\"[Title/Abstract])+AND+(\"Neuroimage\"[Title/Abstract]+OR+\"Magnetic+Resonance+Image\"[Title/Abstract])+AND+(\"confidence+interval\"[Title/Abstract]+OR+\"Estimation\"[Title/Abstract]))&page=1\n",
      "Scraping: https://pubmed.ncbi.nlm.nih.gov/?term=((\"Topological+data+analysis\"[Title/Abstract]+OR+\"landscape+persistence+diagrams\"[Title/Abstract])+AND+(\"Neuroimage\"[Title/Abstract]+OR+\"Magnetic+Resonance+Image\"[Title/Abstract])+AND+(\"confidence+interval\"[Title/Abstract]+OR+\"Estimation\"[Title/Abstract]))&page=2\n",
      "Scraping: https://pubmed.ncbi.nlm.nih.gov/?term=((\"Topological+data+analysis\"[Title/Abstract]+OR+\"landscape+persistence+diagrams\"[Title/Abstract])+AND+(\"Neuroimage\"[Title/Abstract]+OR+\"Magnetic+Resonance+Image\"[Title/Abstract])+AND+(\"confidence+interval\"[Title/Abstract]+OR+\"Estimation\"[Title/Abstract]))&page=3\n",
      "Resultados guardados en pubmed_results2.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Función para construir la URL de búsqueda en PubMed\n",
    "def build_pubmed_url(query, page=1):\n",
    "    base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
    "    return f\"{base_url}?term={query.replace(' ', '+')}&page={page}\"\n",
    "\n",
    "# Encabezados para simular un navegador\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Clave ajustada para PubMed\n",
    "query = '((\"Topological data analysis\"[Title/Abstract] OR \"landscape persistence diagrams\"[Title/Abstract]) AND (\"Neuroimage\"[Title/Abstract] OR \"Magnetic Resonance Image\"[Title/Abstract]) AND (\"confidence interval\"[Title/Abstract] OR \"Estimation\"[Title/Abstract]))'\n",
    "results = []\n",
    "\n",
    "# Iterar sobre las páginas de resultados\n",
    "for page in range(1, 4):  # Cambia 4 al número de páginas que quieras procesar\n",
    "    url = build_pubmed_url(query, page)\n",
    "    print(f\"Scraping: {url}\")\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Encontrar todos los artículos en la página\n",
    "        articles = soup.select(\".docsum-content\")\n",
    "        for article in articles:\n",
    "            # Título del artículo\n",
    "            title_element = article.select_one(\".docsum-title\")\n",
    "            title = title_element.text.strip() if title_element else \"No title available\"\n",
    "            \n",
    "            # Enlace al artículo\n",
    "            link = f\"https://pubmed.ncbi.nlm.nih.gov{title_element['href']}\" if title_element else None\n",
    "            \n",
    "            # Autores (si están disponibles)\n",
    "            authors_element = article.select_one(\".docsum-authors.full-authors\")\n",
    "            authors = authors_element.text.strip() if authors_element else \"No authors available\"\n",
    "            \n",
    "            # Resumen (no está directamente en la lista de resultados, requiere visitar el enlace)\n",
    "            snippet = \"Abstract not available in search results\"\n",
    "            \n",
    "            # Guardar el resultado\n",
    "            results.append({\n",
    "                \"Title\": title,\n",
    "                \"Link\": link,\n",
    "                \"Authors\": authors,\n",
    "                \"Snippet\": snippet\n",
    "            })\n",
    "        \n",
    "        time.sleep(2)  # Pausa entre solicitudes para evitar ser bloqueado\n",
    "    else:\n",
    "        print(f\"Error al acceder a la página: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "# Guardar resultados en un archivo CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"pubmed_results2.csv\", index=False)\n",
    "print(\"Resultados guardados en pubmed_results2.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-scholarly",
   "language": "python",
   "name": "scholarly_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
